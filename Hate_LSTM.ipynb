{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45G9OpK9zkE9",
        "outputId": "17458594-a06e-4b41-811c-34a330da1f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "PvmXS-A8zuYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/naurosromim/bdshs/data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfh5nLPzzvz4",
        "outputId": "84fd2e91-64b0-4ac9-cdae-e5edd8e5e476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading bdshs.zip to ./bdshs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.23M/2.23M [00:00<00:00, 3.74MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS3-86hfzxGl",
        "outputId": "008df237-af31-425b-be4b-74fef06fd545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "deQMY29b0dnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F700-\\U0001F77F\"\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "tZ69AZFl9grR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_init= pd.read_csv(\"/content/bdshs/train.csv\")"
      ],
      "metadata": {
        "id": "PvwrtL2u9hG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take 12k rows randomly from the train dataset\n",
        "df = df_init.sample(n=12000, random_state=42)\n",
        "\n",
        "# Reset the index to start from 0\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Display the first few rows of the new dataset\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk0eCnmu9iYo",
        "outputId": "50d1fbd1-2682-423c-b667-427167ca5463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            sentence target     type  \\\n",
            "0  এখানে অবশ্যই মুন্নি জড়িত সন্ত্রাসীরা ওকে এভাব...    NaN      NaN   \n",
            "1               আসহাই কাকে বলে হেপিকে দেখলে ভুজা জাই    NaN      NaN   \n",
            "2  অপেক্ষায় ছিলাম এই রোস্টের জন্যে ধন্যবাদ তাহসিন...    NaN      NaN   \n",
            "3  উউছিত কাজ করছে সরকার আইন সবার জন্য সমান তুমি স...    NaN      NaN   \n",
            "4  তুমি শালা আর কিছু পেলেনা তাইনা কত চাইছিলা যা দ...   male  slander   \n",
            "\n",
            "   hate speech  \n",
            "0            0  \n",
            "1            0  \n",
            "2            0  \n",
            "3            0  \n",
            "4            1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify columns to drop\n",
        "columns_to_drop = ['target', 'type']\n",
        "\n",
        "# Drop the specified columns\n",
        "# train_data = random_train_data.drop(columns=columns_to_drop)\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Display the DataFrame after dropping columns\n",
        "print(\"\\nDataFrame after dropping specified columns:\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1yjM1ql9jgw",
        "outputId": "826ebed6-717b-4634-88fa-cbd619a9688a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after dropping specified columns:\n",
            "                                                sentence  hate speech\n",
            "0      এখানে অবশ্যই মুন্নি জড়িত সন্ত্রাসীরা ওকে এভাব...            0\n",
            "1                   আসহাই কাকে বলে হেপিকে দেখলে ভুজা জাই            0\n",
            "2      অপেক্ষায় ছিলাম এই রোস্টের জন্যে ধন্যবাদ তাহসিন...            0\n",
            "3      উউছিত কাজ করছে সরকার আইন সবার জন্য সমান তুমি স...            0\n",
            "4      তুমি শালা আর কিছু পেলেনা তাইনা কত চাইছিলা যা দ...            1\n",
            "...                                                  ...          ...\n",
            "11995  নাস্তিক পীর ভন্ড পীর থেকে হে দেশ বাসী সাবধান।ত...            1\n",
            "11996  যে এইটা ভিডিও করেছে সে ত্য এই পোলার থেকেও বড় ব...            1\n",
            "11997                     সেফাত উল্লাকে কেউ মেনশন দেন তো            0\n",
            "11998  সালা আজকালকার মেয়ে গুলাকে বিয়ে করাই ঠিক না ন...            1\n",
            "11999  পাপনের পদত্যাগ চাই ভাই আমরা।হ্যা ভাই সাকিব না ...            0\n",
            "\n",
            "[12000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the remove_emojis function to the 'Text' column\n",
        "df['sentence'] = df['sentence'].apply(remove_emojis)\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(\"\\nDataFrame after removing emojis:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxkrZWRt9lLU",
        "outputId": "314242ed-7ac6-4d2b-c18e-63fb3f9785e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after removing emojis:\n",
            "                                                sentence  hate speech\n",
            "0      এখানে অবশ্যই মুন্নি জড়িত সন্ত্রাসীরা ওকে এভাব...            0\n",
            "1                   আসহাই কাকে বলে হেপিকে দেখলে ভুজা জাই            0\n",
            "2      অপেক্ষায় ছিলাম এই রোস্টের জন্যে ধন্যবাদ তাহসিন...            0\n",
            "3      উউছিত কাজ করছে সরকার আইন সবার জন্য সমান তুমি স...            0\n",
            "4      তুমি শালা আর কিছু পেলেনা তাইনা কত চাইছিলা যা দ...            1\n",
            "...                                                  ...          ...\n",
            "11995  নাস্তিক পীর ভন্ড পীর থেকে হে দেশ বাসী সাবধান।ত...            1\n",
            "11996  যে এইটা ভিডিও করেছে সে ত্য এই পোলার থেকেও বড় ব...            1\n",
            "11997                     সেফাত উল্লাকে কেউ মেনশন দেন তো            0\n",
            "11998  সালা আজকালকার মেয়ে গুলাকে বিয়ে করাই ঠিক না ন...            1\n",
            "11999  পাপনের পদত্যাগ চাই ভাই আমরা।হ্যা ভাই সাকিব না ...            0\n",
            "\n",
            "[12000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "EvV9FSv89mui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "X = df['sentence'].values\n",
        "y = df['hate speech'].values"
      ],
      "metadata": {
        "id": "aXD8GlVZi7Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences\n",
        "max_words = 50000  # Adjust based on your dataset\n",
        "max_len = 128     # Adjust based on your dataset\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(X_seq, maxlen=max_len)"
      ],
      "metadata": {
        "id": "RttKD_rt9upi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n"
      ],
      "metadata": {
        "id": "igVFVnOnjEzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_encoded, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "XC-NPFtF91qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "embedding_dim = 50  # Adjust based on your dataset\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "VcsDFXM_94c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "UNoiJqzr98X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 10  # Adjust based on your dataset\n",
        "batch_size = 32  # Adjust based on your dataset\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)"
      ],
      "metadata": {
        "id": "futAysbF-MjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53443742-fbce-42ee-94bf-4be3589517c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "240/240 [==============================] - 3s 13ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 1.3072 - val_accuracy: 0.7974\n",
            "Epoch 2/10\n",
            "240/240 [==============================] - 2s 10ms/step - loss: 7.1159e-04 - accuracy: 1.0000 - val_loss: 1.3223 - val_accuracy: 0.7922\n",
            "Epoch 3/10\n",
            "240/240 [==============================] - 2s 10ms/step - loss: 5.0906e-04 - accuracy: 0.9999 - val_loss: 1.5841 - val_accuracy: 0.7979\n",
            "Epoch 4/10\n",
            "240/240 [==============================] - 2s 9ms/step - loss: 2.7656e-04 - accuracy: 0.9999 - val_loss: 1.4778 - val_accuracy: 0.7911\n",
            "Epoch 5/10\n",
            "240/240 [==============================] - 3s 12ms/step - loss: 2.7008e-04 - accuracy: 0.9999 - val_loss: 1.5654 - val_accuracy: 0.7948\n",
            "Epoch 6/10\n",
            "240/240 [==============================] - 3s 12ms/step - loss: 1.7837e-04 - accuracy: 1.0000 - val_loss: 1.5659 - val_accuracy: 0.7953\n",
            "Epoch 7/10\n",
            "240/240 [==============================] - 2s 9ms/step - loss: 1.8549e-04 - accuracy: 1.0000 - val_loss: 1.5998 - val_accuracy: 0.7953\n",
            "Epoch 8/10\n",
            "240/240 [==============================] - 2s 10ms/step - loss: 2.5812e-04 - accuracy: 0.9999 - val_loss: 1.5506 - val_accuracy: 0.7932\n",
            "Epoch 9/10\n",
            "240/240 [==============================] - 2s 10ms/step - loss: 4.0142e-04 - accuracy: 0.9999 - val_loss: 1.6932 - val_accuracy: 0.7932\n",
            "Epoch 10/10\n",
            "240/240 [==============================] - 2s 10ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.4047 - val_accuracy: 0.7823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "iApYm5FU-Ovh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0b9e97-93d7-4a08-85ce-d241a02412ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 0s 5ms/step - loss: 1.3697 - accuracy: 0.7925\n",
            "Test Loss: 1.3697, Test Accuracy: 0.7925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of making predictions\n",
        "sample_text = [\"আইসিসিকে এই তথ্য দিলো কে?শুওরের বাচ্চা পাপন\"]\n",
        "sample_seq = tokenizer.texts_to_sequences(sample_text)\n",
        "sample_pad = pad_sequences(sample_seq, maxlen=max_len)\n",
        "prediction = model.predict(sample_pad)\n",
        "predicted_label = \"Hate Speech\" if prediction[0] >= 0.5 else \"Not Hate Speech\"\n",
        "print(f'Predicted Label: {predicted_label}')"
      ],
      "metadata": {
        "id": "4IPAqR78-RFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a6224f-ab5c-45de-a06f-98d01002bb78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted Label: Hate Speech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('hate_speech.h5')  # Save the model as an HDF5 file\n"
      ],
      "metadata": {
        "id": "_kHwIChcrrbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the tokenizer to a file using pickle\n",
        "tokenizer_save_path = 'tokenizer.pickle'\n",
        "with open(tokenizer_save_path, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "z6PlGh-UwKxN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}